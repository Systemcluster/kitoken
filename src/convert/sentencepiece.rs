#[cfg(feature = "std")]
use std::fs::File;
#[cfg(feature = "std")]
use std::io::Read;
#[cfg(feature = "std")]
use std::path::Path;

use alloc::format;
use alloc::string::ToString;
use alloc::vec::Vec;
use core::cmp::Ordering;

use hashbrown::HashMap;
use sentencepiece_model::{ModelType, SentencePiece, SentencePieceModel, Type};

use crate::convert::ConversionError;
use crate::{Configuration, Definition, Kitoken, Mode, Scores, UnicodeNormalization, Vocab};

#[derive(Debug)]
struct ParsedPiece {
    index: u32,
    score: f32,
    piece: SentencePiece,
}

/// Converts a `sentencepiece` model into the definition format used by this crate.
///
/// `data` is the raw model data generated by the `sentencepiece` tokenizer.
///
/// Returns the tokenizer definition, or an error if the conversion fails.
///
/// # Examples
///
/// ```
/// use kitoken::convert::convert_sentencepiece;
/// use kitoken::Kitoken;
///
/// let data = include_bytes!("../../tests/models/llama2.model");
/// let definition = convert_sentencepiece(data).unwrap();
///
/// let tokenizer = Kitoken::try_from(definition).unwrap();
/// ```
///
/// Additional conversion utilities are defined in [`Definition`] and [`Kitoken`].
///
/// # Format
///
/// SentencePiece models are used and generated by the `sentencepiece` tokenizer.
///
/// SentencePiece models can contain different model types, including `BPE`, `Unigram`, `Char` and `Word`.
/// This function supports conversion of `BPE` and `Unigram` models.
pub fn convert_sentencepiece(data: impl AsRef<[u8]>) -> Result<Definition, ConversionError> {
    let data = data.as_ref();
    let model = SentencePieceModel::from_slice(data).map_err(|e| {
        ConversionError::InvalidData(format!("failed to parse sentencepiece model: {:?}", e))
    })?;
    convert_sentencepiece_model(model)
}
fn convert_sentencepiece_model(model: SentencePieceModel) -> Result<Definition, ConversionError> {
    let mut config = Configuration {
        ..Configuration::default()
    };

    let mut model_type = ModelType::Unigram;
    if let Some(trainer) = model.trainer() {
        let mut splits = Vec::new();
        if trainer.treat_whitespace_as_suffix() {
            splits.push(r"[^\t\n\f\r ]+[\t\n\f\r ]+".to_string());
        } else {
            splits.push(r"[\t\n\f\r ]+[^\t\n\f\r ]+".to_string());
        }
        config.split = splits.join("|");
        config.specials.unk =
            Some((trainer.unk_id() as u32, trainer.unk_surface().as_bytes().to_vec()));
        config.specials.bos =
            Some((trainer.bos_id() as u32, trainer.bos_piece().as_bytes().to_vec()));
        config.specials.eos =
            Some((trainer.eos_id() as u32, trainer.eos_piece().as_bytes().to_vec()));
        config.specials.pad =
            Some((trainer.pad_id() as u32, trainer.pad_piece().as_bytes().to_vec()));
        model_type = trainer.model_type();
    } else {
        config.split = r"[\t\n\f\r ]+[^\t\n\f\r ]+".to_string();
    }

    match model_type {
        ModelType::Bpe => config.mode = Mode::CharPair,
        ModelType::Unigram => config.mode = Mode::Unigram,
        _ => {
            return Err(ConversionError::InvalidData(format!(
                "unsupported model type: {:?}",
                model_type
            )));
        }
    }

    if model.pieces.len() > u32::MAX as usize {
        return Err(ConversionError::InvalidData(format!(
            "too many pieces: {}",
            model.pieces.len()
        )));
    }

    if let Some(normalizer) = model.normalizer() {
        config.normalization.unicode = match normalizer.name() {
            "nmt_nfkc" => UnicodeNormalization::NFKCNMT,
            "nfkc" => UnicodeNormalization::NFKC,
            "nmt_nfkc_cf" => UnicodeNormalization::NFKCNMTCF,
            "nfkc_cf" => UnicodeNormalization::NFKCCF,
            _ => UnicodeNormalization::None,
        };
        config.normalization.trim_whitespace = normalizer.remove_extra_whitespaces();
        config.normalization.collapse_whitespace = normalizer.remove_extra_whitespaces();
        config.normalization.collapse_unknown = normalizer.remove_extra_whitespaces();
        config.normalization.prefix_whitespace = normalizer.add_dummy_prefix();
    }

    let mut vocab = HashMap::<Vec<u8>, ParsedPiece>::with_capacity(model.pieces.len());
    let mut specials = HashMap::<Vec<u8>, ParsedPiece>::default();

    for (index, piece) in model.pieces.iter().enumerate() {
        let text = piece
            .piece
            .as_ref()
            .ok_or_else(|| ConversionError::InvalidData(format!("piece {} has no text", index)))?;
        let piece_type = piece.r#type();

        let text = if piece_type == Type::Byte {
            // byte encoding in the form `<0xAA>`
            let rune = &text[3..5];
            let rune = u32::from_str_radix(rune, 16)
                .map_err(|e| ConversionError::InvalidNumber(format!("{:?}", e)))?;
            [rune as u8].to_vec()
        } else {
            text.to_string().replace('â–', " ").as_bytes().to_vec()
        };

        if piece_type == Type::UserDefined || piece_type == Type::Control {
            specials.insert(text, ParsedPiece {
                index: index as u32,
                score: index as f32,
                piece: piece.clone(),
            });
            continue;
        }

        if let Some(existing) = vocab.get(&text) {
            let existing_type = existing.piece.r#type();
            if piece_type == Type::Byte && existing_type != Type::Byte {
                continue;
            }
        }

        vocab.insert(text, ParsedPiece {
            index: index as u32,
            score: piece.score(),
            piece: piece.clone(),
        });
    }

    let (vocab, specials, scores) = match model_type {
        ModelType::Bpe => {
            let mut vocab_merges = HashMap::<u32, f32>::with_capacity(vocab.len() * 3);
            for (text, piece) in vocab.iter() {
                for split in 1..text.len() {
                    let left = &text[..split];
                    let right = &text[split..];
                    if let (Some(_), Some(_)) = (vocab.get(left), vocab.get(right)) {
                        vocab_merges.insert(piece.index, piece.score);
                    }
                }
            }
            let mut vocab =
                vocab.into_iter().map(|(text, piece)| (text, piece.index)).collect::<Vocab>();
            vocab.sort_by(|(_, a), (_, b)| {
                if let (Some(ma), Some(mb)) = (vocab_merges.get(a), vocab_merges.get(b)) {
                    let comp = mb.partial_cmp(ma).unwrap();
                    if comp == Ordering::Equal {
                        a.partial_cmp(b).unwrap()
                    } else {
                        comp
                    }
                } else if vocab_merges.get(a).is_some() {
                    Ordering::Less
                } else if vocab_merges.get(b).is_some() {
                    Ordering::Greater
                } else {
                    Ordering::Equal
                }
            });
            let mut specials_merges = HashMap::<u32, f32>::with_capacity(specials.len() * 3);
            for (text, piece) in specials.iter() {
                for split in 1..text.len() {
                    let left = &text[..split];
                    let right = &text[split..];
                    if let (Some(_), Some(_)) = (specials.get(left), specials.get(right)) {
                        specials_merges.insert(piece.index, piece.score);
                    }
                }
            }
            let mut specials =
                specials.into_iter().map(|(text, piece)| (text, piece.index)).collect::<Vocab>();
            specials.sort_by(|(_, a), (_, b)| {
                if let (Some(ma), Some(mb)) = (specials_merges.get(a), specials_merges.get(b)) {
                    let comp = mb.partial_cmp(ma).unwrap();
                    if comp == Ordering::Equal {
                        a.partial_cmp(b).unwrap()
                    } else {
                        comp
                    }
                } else if specials_merges.get(a).is_some() {
                    Ordering::Less
                } else if specials_merges.get(b).is_some() {
                    Ordering::Greater
                } else {
                    Ordering::Equal
                }
            });
            let scores = Scores::with_capacity(0);
            (vocab, specials, scores)
        }
        ModelType::Unigram => {
            let mut vocab = vocab.into_iter().collect::<Vec<_>>();
            vocab.sort_by(|(_, a), (_, b)| a.score.partial_cmp(&b.score).unwrap());
            let scores = vocab.iter().map(|(_, piece)| piece.score).collect::<Scores>();
            let vocab =
                vocab.into_iter().map(|(text, piece)| (text, piece.index)).collect::<Vocab>();
            let mut specials = specials.into_iter().collect::<Vec<_>>();
            specials.sort_by(|(_, a), (_, b)| a.score.partial_cmp(&b.score).unwrap());
            let specials =
                specials.into_iter().map(|(text, piece)| (text, piece.index)).collect::<Vocab>();
            (vocab, specials, scores)
        }
        _ => unreachable!(),
    };

    Ok(Definition {
        vocab,
        specials,
        scores,
        config,
    })
}

impl Definition {
    /// Converts a `sentencepiece` model into the encoder format used by this crate.
    /// See [`convert_sentencepiece`] for more details.
    #[cfg(feature = "std")]
    pub fn from_sentencepiece_reader<R: Read>(reader: &mut R) -> Result<Self, ConversionError> {
        let mut data = Vec::with_capacity(1024);
        reader.read_to_end(&mut data)?;
        Self::from_sentencepiece_slice(&data)
    }

    /// Converts a `sentencepiece` model into the encoder format used by this crate.
    /// See [`convert_sentencepiece`] for more details.
    #[cfg(feature = "std")]
    pub fn from_sentencepiece_file(path: impl AsRef<Path>) -> Result<Self, ConversionError> {
        let mut file = File::open(path)?;
        Self::from_sentencepiece_reader(&mut file)
    }

    /// Converts a `sentencepiece` model into the encoder format used by this crate.
    /// See [`convert_sentencepiece`] for more details.
    pub fn from_sentencepiece_slice(data: &[u8]) -> Result<Self, ConversionError> {
        convert_sentencepiece(data)
    }

    /// Converts a `sentencepiece` model into the encoder format used by this crate.
    /// See [`convert_sentencepiece`] for more details.
    pub fn from_sentencepiece_model(model: SentencePieceModel) -> Result<Self, ConversionError> {
        convert_sentencepiece_model(model)
    }
}

impl Kitoken {
    /// Initializes the tokenizer from a `sentencepiece` model.
    /// See [`convert_sentencepiece`] for more details.
    #[cfg(feature = "std")]
    pub fn from_sentencepiece_reader<R: Read>(reader: &mut R) -> Result<Self, ConversionError> {
        Ok(Self::from_definition(Definition::from_sentencepiece_reader(reader)?)?)
    }

    /// Initializes the tokenizer from a `sentencepiece` model.
    /// See [`convert_sentencepiece`] for more details.
    #[cfg(feature = "std")]
    pub fn from_sentencepiece_file(path: impl AsRef<Path>) -> Result<Self, ConversionError> {
        Ok(Self::from_definition(Definition::from_sentencepiece_file(path)?)?)
    }

    /// Initializes the tokenizer from a `sentencepiece` model.
    /// See [`convert_sentencepiece`] for more details.
    pub fn from_sentencepiece_slice(data: &[u8]) -> Result<Self, ConversionError> {
        Ok(Self::from_definition(Definition::from_sentencepiece_slice(data)?)?)
    }

    /// Initializes the tokenizer from a `sentencepiece` model.
    /// See [`convert_sentencepiece`] for more details.
    pub fn from_sentencepiece_model(model: SentencePieceModel) -> Result<Self, ConversionError> {
        Ok(Self::from_definition(Definition::from_sentencepiece_model(model)?)?)
    }
}
